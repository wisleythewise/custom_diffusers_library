{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/diffusers/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/diffusers/lib/python3.8/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Keyword arguments {'ignore_mismatched_sizes': False} are not expected by StableVideoDiffusionPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  8.20it/s]\n",
      "/root/miniconda3/envs/diffusers/lib/python3.8/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 12.87it/s]\n",
      "Keyword arguments {'ignore_mismatched_sizes': False} are not expected by StableVideoDiffusionPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE scale factor: 8\n"
     ]
    }
   ],
   "source": [
    "import debugpy\n",
    "import gc\n",
    "from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from typing import Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from diffusers.utils.torch_utils import is_compiled_module, randn_tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.loaders import UNet2DConditionLoadersMixin\n",
    "from diffusers.utils import BaseOutput, logging\n",
    "from diffusers.models.attention_processor import CROSS_ATTENTION_PROCESSORS, AttentionProcessor, AttnProcessor\n",
    "from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.unets.unet_3d_blocks import UNetMidBlockSpatioTemporal, get_down_block, get_up_block\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet,SpatioTemporalControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n",
    "\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "\n",
    "from types import MethodType\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16\n",
    "time =  torch.tensor(1).to(dtype=dtype)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "from typing import Optional, List\n",
    "\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet,SpatioTemporalControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n",
    "\n",
    "import gc\n",
    "from diffusers import DiffusionPipeline\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "\n",
    "class DiffusionDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((320, 512)),\n",
    "            transforms.CenterCrop((320, 512)),\n",
    "        ])\n",
    "        self.image_processor = VaeImageProcessor(vae_scale_factor=8)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming each set of ground truths represents a separate sample\n",
    "        return len(self.data['ground_truth'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Processing ground truth images\n",
    "        \n",
    "        ground_truth_images = [self.transform(Image.open(path)) for path in self.data['ground_truth'][idx]]\n",
    "        ground_truth_images = self.image_processor.preprocess(image = ground_truth_images, height = 320, width = 512)\n",
    "\n",
    "        # Processing conditioning images set one (assuming RGB, 4 channels after conversion)\n",
    "        conditioning_images_one = [self.transform(Image.open(path).convert(\"RGB\")) for path in self.data['conditioning_images_one'][idx]]\n",
    "        conditioning_images_one = self.image_processor.preprocess(image = conditioning_images_one, height = 320, width = 512)\n",
    "\n",
    "        # Processing conditioning images set two (assuming grayscale, converted to RGB to match dimensions)\n",
    "        # conditioning_images_two = [self.transform(Image.open(path)) for path in self.data['conditioning_images_two'][idx]]\n",
    "        # conditioning_images_two = self.image_processor.preprocess(image = conditioning_images_two, height = 320, width = 512)\n",
    "        \n",
    "        # Concatenating condition one and two images along the channel dimension\n",
    "        # conditioned_images = [torch.cat((img_one, img_two), dim=0) for img_one, img_two in zip(conditioning_images_one, conditioning_images_two)]\n",
    "\n",
    "        # Processing reference images (single per scene, matched by index)\n",
    "        reference_image = self.transform(Image.open(self.data['ground_truth'][idx][0]))\n",
    "\n",
    "        # Retrieving the corresponding caption\n",
    "        caption = self.data['caption'][idx][0]\n",
    "\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"ground_truth\": ground_truth_images,\n",
    "            \"conditioning\": conditioning_images_one,\n",
    "            \"caption\": caption,\n",
    "            \"reference_image\": reference_image\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ground_truth = torch.stack([item['ground_truth'] for item in batch])\n",
    "    conditioning = torch.stack([item['conditioning'] for item in batch])\n",
    "    captions = [item['caption'] for item in batch]  # List of strings, no need to stack\n",
    "    reference_images = [item['reference_image'] for item in batch]\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"ground_truth\": ground_truth.flatten(0, 1),\n",
    "        \"conditioning\": conditioning.flatten(0, 1),\n",
    "        \"caption\": captions[0],\n",
    "        \"reference_image\": reference_images[0],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = DiffusionDataset(json_path='/home/wisley/custom_diffusers_library/src/diffusers/jasper/validation.json')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=1,  # Or your preferred batch size\n",
    "    num_workers=0,  # Adjust based on your setup\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "data_iter = iter(train_dataloader)\n",
    "batch = next(data_iter)\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet,SpatioTemporalControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n",
    "prompt = \"A driving scene during the night, with rainy weather in boston-seaport\"\n",
    "# prompt = batch['caption']\n",
    "pseudo_sample = batch['conditioning'][:14]\n",
    "# Define a simple torch generator\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "image = batch['reference_image']\n",
    "\n",
    "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid\", torch_dtype=torch.float16, variant=\"fp16\", ignore_mismatched_sizes=False\n",
    ")\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2\")\n",
    "\n",
    "\n",
    "# Getting the models\n",
    "tokenizer = pipeline.tokenizer\n",
    "text_encoder = pipeline.text_encoder\n",
    "unet_weights = pipe.unet.state_dict()\n",
    "testing_unet = UNetSpatioTemporalConditionModel()\n",
    "testing_unet.load_state_dict(unet_weights)\n",
    "control_net_blank = SpatioTemporalControlNet.from_unet(testing_unet).half()\n",
    "testing_unet = testing_unet.half()\n",
    "\n",
    "pipe_zero = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid\", torch_dtype=torch.float16, variant=\"fp16\", ignore_mismatched_sizes=False\n",
    ")\n",
    "\n",
    "pipe_zero_with_controlnet = StableVideoDiffusionPipelineWithControlNet(\n",
    "    vae = pipe.vae,\n",
    "    image_encoder = pipe.image_encoder,\n",
    "    unet=pipe_zero.unet.to(dtype=torch.float16, device=torch.device(\"cuda\")),\n",
    "    scheduler=pipe.scheduler,\n",
    "    feature_extractor=pipe.feature_extractor,\n",
    "    controlnet=control_net_blank,\n",
    "    tokenizer = tokenizer,\n",
    "    text_encoder = text_encoder\n",
    ")\n",
    "\n",
    "pipe_zero_with_controlnet = pipe_zero_with_controlnet.to(dtype=torch.float16, device=torch.device(\"cuda\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]/root/miniconda3/envs/diffusers/lib/python3.8/site-packages/torch/nn/modules/conv.py:605: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv3d(\n",
      "/root/miniconda3/envs/diffusers/lib/python3.8/site-packages/diffusers/models/lora.py:358: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(\n",
      "100%|██████████| 25/25 [00:13<00:00,  1.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pod.mp4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_zero_with_controlnet = pipe_zero_with_controlnet(height=320,width=512, image=image,num_frames = 14, prompt=prompt, conditioning_image = pseudo_sample,  decode_chunk_size=8, generator=generator).frames[0]\n",
    "export_to_video(frames_zero_with_controlnet, \"pod.mp4\", fps=7)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
