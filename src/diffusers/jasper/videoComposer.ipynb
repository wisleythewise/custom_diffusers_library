{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 has 14 samples\n",
      "Caption: A driving scene during the day, with clear weather in singapore-onenorth\n",
      "Reference image shape: torch.Size([3, 320, 512])\n",
      "Conditioning image shape: torch.Size([14, 4, 320, 512])\n",
      "Batch 1 has 14 samples\n",
      "Caption: A driving scene during the day, with clear weather in singapore-onenorth\n",
      "Reference image shape: torch.Size([3, 320, 512])\n",
      "Conditioning image shape: torch.Size([14, 4, 320, 512])\n",
      "Batch 2 has 14 samples\n",
      "Caption: A driving scene during the day, with clear weather in singapore-onenorth\n",
      "Reference image shape: torch.Size([3, 320, 512])\n",
      "Conditioning image shape: torch.Size([14, 4, 320, 512])\n"
     ]
    }
   ],
   "source": [
    "import debugpy\n",
    "import gc\n",
    "from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from typing import Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from diffusers.utils.torch_utils import is_compiled_module, randn_tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.loaders import UNet2DConditionLoadersMixin\n",
    "from diffusers.utils import BaseOutput, logging\n",
    "from diffusers.models.attention_processor import CROSS_ATTENTION_PROCESSORS, AttentionProcessor, AttnProcessor\n",
    "from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.unets.unet_3d_blocks import UNetMidBlockSpatioTemporal, get_down_block, get_up_block\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet,SpatioTemporalControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import wrapperModel,StableVideoDiffusionPipelineWithWrapper, StableVideoDiffusionPipelineWithControlNet,StableVideoDiffusionPipelineWithControlNet, SpatioTemporalControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n",
    "\n",
    "import gc\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "from types import MethodType\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "\n",
    "class DiffusionDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((320, 512)),\n",
    "            transforms.CenterCrop((320, 512)),\n",
    "        ])\n",
    "        self.image_processor = VaeImageProcessor(vae_scale_factor=8)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming each set of ground truths represents a separate sample\n",
    "        return len(self.data['ground_truth'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Processing ground truth images\n",
    "        \n",
    "        ground_truth_images = [self.transform(Image.open(path)) for path in self.data['ground_truth'][idx]]\n",
    "        ground_truth_images = self.image_processor.preprocess(image = ground_truth_images, height = 320, width = 512)\n",
    "\n",
    "        # Processing conditioning images set one (assuming RGB, 4 channels after conversion)\n",
    "        conditioning_images_one = [self.transform(Image.open(path)) for path in self.data['conditioning_images_one'][idx]]\n",
    "        conditioning_images_one = self.image_processor.preprocess(image = conditioning_images_one, height = 320, width = 512)\n",
    "\n",
    "        # Processing conditioning images set two (assuming grayscale, converted to RGB to match dimensions)\n",
    "        conditioning_images_two = [self.transform(Image.open(path)) for path in self.data['conditioning_images_two'][idx]]\n",
    "        conditioning_images_two = self.image_processor.preprocess(image = conditioning_images_two, height = 320, width = 512)\n",
    "        \n",
    "        # Concatenating condition one and two images along the channel dimension\n",
    "        conditioned_images = [torch.cat((img_one, img_two), dim=0) for img_one, img_two in zip(conditioning_images_one, conditioning_images_two)]\n",
    "\n",
    "        # Processing reference images (single per scene, matched by index)\n",
    "        reference_image = self.transform(Image.open(self.data['reference_image'][idx][0]))\n",
    "\n",
    "        # Retrieving the corresponding caption\n",
    "        caption = self.data['caption'][idx][0]\n",
    "\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"ground_truth\": ground_truth_images,\n",
    "            \"conditioning\": torch.stack(conditioned_images),\n",
    "            \"caption\": caption,\n",
    "            \"reference_image\": reference_image\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ground_truth = torch.stack([item['ground_truth'] for item in batch])\n",
    "    conditioning = torch.stack([item['conditioning'] for item in batch])\n",
    "    captions = [item['caption'] for item in batch]  # List of strings, no need to stack\n",
    "    reference_images = [item['reference_image'] for item in batch]\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"ground_truth\": ground_truth.flatten(0, 1),\n",
    "        \"conditioning\": conditioning.flatten(0, 1),\n",
    "        \"caption\": captions[0],\n",
    "        \"reference_image\": reference_images[0],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = DiffusionDataset(json_path='/home/wisley/custom_diffusers_library/src/diffusers/jasper/complete_data_paths.json')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=1,  # Or your preferred batch size\n",
    "    num_workers=0,  # Adjust based on your setup\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    print(f\"Batch {i} has {batch['ground_truth'].shape[0]} samples\")\n",
    "    print(f\"Caption: {batch['caption']}\")\n",
    "    print(f\"Reference image shape: {to_tensor(batch['reference_image']).shape}\")\n",
    "    print(f\"Conditioning image shape: {batch['conditioning'].shape}\")\n",
    "\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'ignore_mismatched_sizes': False} are not expected by StableVideoDiffusionPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:01<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE scale factor: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:00<00:10,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:00<00:09,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [00:01<00:09,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [00:01<00:08,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:02<00:08,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [00:02<00:08,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [00:02<00:07,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [00:03<00:07,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [00:03<00:06,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [00:04<00:06,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [00:04<00:05,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [00:05<00:05,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [00:05<00:05,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [00:05<00:04,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [00:06<00:04,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [00:06<00:03,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [00:07<00:03,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [00:07<00:02,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [00:08<00:02,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [00:08<00:02,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [00:08<00:01,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [00:09<00:01,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [00:09<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [00:10<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this is the shape of the latent model input: torch.Size([2, 14, 4, 40, 64]) \n",
      "this si teh shape of the conditioning torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x torch.Size([2, 14, 4, 40, 64])\n",
      "this si teh shape of the x after the concatenation torch.Size([2, 14, 8, 40, 64])\n",
      " this si the value of the noise pred: torch.Size([2, 14, 4, 40, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:10<00:00,  2.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nolatents@.mp4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the pipelines\n",
    "\n",
    "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid\", torch_dtype=torch.float16, variant=\"fp16\", ignore_mismatched_sizes=False\n",
    ")\n",
    "config = {\n",
    "    \"output_size\": (40, 64), \n",
    "    \"num_channels\": 4\n",
    "}\n",
    "# Create a wrapper model\n",
    "custom_conditioning_net = CustomConditioningNet(**config)\n",
    "model = pipe.unet\n",
    "wrapper_model = wrapperModel(customConditioningNet=custom_conditioning_net, model = model)\n",
    "# Create a wrapper pipeline\n",
    "pipe_with_wrapper = StableVideoDiffusionPipelineWithWrapper(\n",
    "    vae = pipe.vae,\n",
    "    image_encoder = pipe.image_encoder,\n",
    "    scheduler=pipe.scheduler,\n",
    "    feature_extractor=pipe.feature_extractor,\n",
    "    wrapper = wrapper_model\n",
    ")\n",
    "\n",
    "prompt = \"A driving scene during the night, with rainy weather in boston-seaport\"\n",
    "# prompt = batch['caption']\n",
    "pseudo_sample = batch['conditioning'].to(dtype=torch.float16, device=torch.device(\"cuda\"))\n",
    "# Define a simple torch generator\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "image = batch['reference_image']\n",
    "\n",
    "pipe = pipe.to(dtype=torch.float16, device=torch.device(\"cuda\"))\n",
    "\n",
    "honden = pipe_with_wrapper(height=320,width=512, image=image,conditioning_image=pseudo_sample ,num_frames = 14,  decode_chunk_size=8, generator=generator).frames[0]\n",
    "export_to_video(honden, \"nolatents@.mp4\", fps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 4, 320, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['conditioning'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsl_diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
