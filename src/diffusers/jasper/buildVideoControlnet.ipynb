{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal unet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/wisley/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import debugpy\n",
    "import gc\n",
    "from transformers import CLIPImageProcessor, CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "from typing import Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from diffusers.utils.torch_utils import is_compiled_module, randn_tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.loaders import UNet2DConditionLoadersMixin\n",
    "from diffusers.utils import BaseOutput, logging\n",
    "from diffusers.models.attention_processor import CROSS_ATTENTION_PROCESSORS, AttentionProcessor, AttnProcessor\n",
    "from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.unets.unet_3d_blocks import UNetMidBlockSpatioTemporal, get_down_block, get_up_block\n",
    "from diffusers.models.unets import UNetSpatioTemporalConditionModel\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet,SpatioTemporalControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n",
    "\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "\n",
    "from types import MethodType\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha = CustomConditioningNet(output_size=(40,60))\n",
    "\n",
    "test_sample = torch.zeros(14,4,320,512)\n",
    "\n",
    "output = ha(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "torch.Size([1, 14, 3, 320, 512])\n",
      "torch.Size([1, 14, 4, 320, 512])\n",
      "A driving scene during the day, with clear weather in boston-seaport\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconditioning\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreference_image\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Image' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "class DiffusionDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.image_factor_x = 360 / 320    \n",
    "        self.image_factor_y = 640 / 512\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((int(360/self.image_factor_y) , int(640/self.image_factor_x))),\n",
    "            transforms.CenterCrop((320, 512)),\n",
    "        ])\n",
    "        self.image_processor = VaeImageProcessor(vae_scale_factor=8)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming each set of ground truths represents a separate sample\n",
    "        return len(self.data['ground_truth'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Processing ground truth images\n",
    "        \n",
    "        ground_truth_images = [self.transform(Image.open(path)) for path in self.data['ground_truth'][idx]]\n",
    "        ground_truth_images = self.image_processor.preprocess(image = ground_truth_images, height = 320, width = 512)\n",
    "\n",
    "        # prescan_images = [self.transform(Image.open(path)) for path in self.data['prescan_images'][idx]]\n",
    "        # prescan_images = self.image_processor.preprocess(image = prescan_images, height = 320, width = 512)\n",
    "\n",
    "        # Processing conditioning images set one (assuming RGB, 4 channels after conversion)\n",
    "        conditioning_images_one = [self.transform(Image.open(path)) for path in self.data['conditioning_images_one'][idx]]\n",
    "        conditioning_images_one = self.image_processor.preprocess(image = conditioning_images_one, height = 320, width = 512)\n",
    "\n",
    "        # Processing conditioning images set two (assuming grayscale, converted to RGB to match dimensions)\n",
    "        # conditioning_images_two = [self.transform(Image.open(path)) for path in self.data['conditioning_images_two'][idx]]\n",
    "        # conditioning_images_two = self.image_processor.preprocess(image = conditioning_images_two, height = 320, width = 512)\n",
    "        \n",
    "        # Concatenating condition one and two images along the channel dimension\n",
    "        # conditioned_images = [torch.cat((img_one, img_two), dim=0) for img_one, img_two in zip(conditioning_images_one, conditioning_images_two)]\n",
    "\n",
    "        # Processing reference images (single per scene, matched by index)\n",
    "        reference_image = self.transform(Image.open(self.data['ground_truth'][idx][0]))\n",
    "\n",
    "        # Retrieving the corresponding caption\n",
    "        caption = self.data['caption'][idx][0]\n",
    "\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"ground_truth\": ground_truth_images,\n",
    "            \"conditioning\": conditioning_images_one,\n",
    "            \"caption\": caption,\n",
    "            \"reference_image\": reference_image,\n",
    "            # \"prescan_images\": prescan_images\n",
    "        }\n",
    "def collate_fn(batch):\n",
    "    ground_truth = torch.stack([item['ground_truth'] for item in batch])\n",
    "    conditioning = torch.stack([item['conditioning'] for item in batch])\n",
    "    captions = [item['caption'] for item in batch]  # List of strings, no need to stack\n",
    "    reference_images = [item['reference_image'] for item in batch]\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"conditioning\": conditioning,\n",
    "        \"caption\": captions[0],\n",
    "        \"reference_image\": reference_images[0],\n",
    "        # \"prescan_images\": prescan_images\n",
    "    }\n",
    "\n",
    "train_dataset = DiffusionDataset(json_path='/home/wisley/custom_diffusers_library/src/diffusers/jasper/jappie_seg.json')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=1,  # Or your preferred batch size\n",
    "    num_workers=0,  # Adjust based on your setup\n",
    ")\n",
    "print(\"x\")\n",
    "for batch in train_dataloader:\n",
    "    print(batch['ground_truth'].shape)\n",
    "    print(batch['conditioning'].shape)\n",
    "    print(batch['caption'])\n",
    "    print(batch['reference_image'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  8.57it/s]\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 14.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prompt embeds: torch.Size([1, 77, 1024]) 1 77\n",
      "Time is on: cuda:0\n",
      "AAAAAAAAAAAAAAAAAA Encoder hidden states shape: torch.Size([28, 1, 1024])\n",
      "AAAAAAAAAAAAAAAAAA Sample shape: torch.Size([28, 320, 36, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16\n",
    "time =  torch.tensor(1).to(dtype=dtype)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "from typing import Optional, List\n",
    "\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet,SpatioTemporalControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n",
    "\n",
    "import gc\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# Importing the pipelines\n",
    "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid\", torch_dtype=torch.float16, variant=\"fp16\"\n",
    ")\n",
    "\n",
    "\n",
    "#  `stable-video-diffusion-img2vid-xt`\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2\")\n",
    "\n",
    "testing_unet = pipeline.unet\n",
    "testing_unet = testing_unet.to(dtype=dtype, device=device)\n",
    "\n",
    "# Getting the models\n",
    "tokenizer = pipeline.tokenizer\n",
    "text_encoder = pipeline.text_encoder\n",
    "unet_weights = pipe.unet.state_dict()\n",
    "my_unet = UNetSpatioTemporalConditionModel()\n",
    "my_unet.load_state_dict(unet_weights)\n",
    "\n",
    "def encode_prompt(\n",
    "    prompt,\n",
    "    device,\n",
    "    do_classifier_free_guidance,\n",
    "    negative_prompt=\"Simulation, artifacts, blurry, low resolution, low quality, noisy, grainy, distorted\",\n",
    "    num_images_per_prompt = 1,\n",
    "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    lora_scale: Optional[float] = None,\n",
    "    clip_skip: Optional[int] = None,\n",
    "    text_encoder = None, \n",
    "    tokenizer = None):\n",
    "    # Set the text_encoder and the tokenizer on the correct device  \n",
    "    text_encoder = text_encoder.to(device)\n",
    "\n",
    "\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "    else:\n",
    "        batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "    if True:\n",
    "\n",
    "        text_inputs = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "        untruncated_ids = tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n",
    "            text_input_ids, untruncated_ids\n",
    "        ):\n",
    "            removed_text = tokenizer.batch_decode(\n",
    "                untruncated_ids[:, tokenizer.model_max_length - 1 : -1]\n",
    "            )\n",
    "            logger.warning(\n",
    "                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
    "                f\" {tokenizer.model_max_length} tokens: {removed_text}\"\n",
    "            )\n",
    "\n",
    "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "            attention_mask = text_inputs.attention_mask.to(device)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "\n",
    "        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=attention_mask)\n",
    "        prompt_embeds = prompt_embeds[0]\n",
    "       \n",
    "\n",
    "    if text_encoder is not None:\n",
    "        prompt_embeds_dtype = text_encoder.dtype\n",
    "\n",
    "\n",
    "    prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "\n",
    "    bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "    print(f\"Shape of prompt embeds: {prompt_embeds.shape} {bs_embed} {seq_len}\")\n",
    "    \n",
    "    # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "    prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "    # get unconditional embeddings for classifier free guidance\n",
    "    if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
    "        uncond_tokens: List[str]\n",
    "        if negative_prompt is None:\n",
    "            uncond_tokens = [\"\"] * batch_size\n",
    "        elif prompt is not None and type(prompt) is not type(negative_prompt):\n",
    "            raise TypeError(\n",
    "                f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                f\" {type(prompt)}.\"\n",
    "            )\n",
    "        elif isinstance(negative_prompt, str):\n",
    "            uncond_tokens = [negative_prompt]\n",
    "        elif batch_size != len(negative_prompt):\n",
    "            raise ValueError(\n",
    "                f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                \" the batch size of `prompt`.\"\n",
    "            )\n",
    "        else:\n",
    "            uncond_tokens = negative_prompt\n",
    "        \n",
    "        max_length = prompt_embeds.shape[1]\n",
    "        uncond_input = tokenizer(\n",
    "            uncond_tokens,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "            attention_mask = uncond_input.attention_mask.to(device)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        negative_prompt_embeds = text_encoder(\n",
    "            uncond_input.input_ids.to(device),\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        negative_prompt_embeds = negative_prompt_embeds[0]\n",
    "\n",
    "    if do_classifier_free_guidance:\n",
    "        # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "        seq_len = negative_prompt_embeds.shape[1]\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "\n",
    "    \n",
    "    embeds = torch.cat([prompt_embeds, negative_prompt_embeds])\n",
    "    embeds = embeds.mean(dim=1, keepdim=True)\n",
    "\n",
    "    return embeds\n",
    "\n",
    "prompt = \"A f a cat\"\n",
    "prompt_embeds = encode_prompt(prompt=prompt, device=device, do_classifier_free_guidance=True, text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "\n",
    "def get_add_time_ids(\n",
    "  fps = 7,\n",
    "  motion_bucket_id = 127,\n",
    "  noise_aug_strength = 0.02,\n",
    "  dtype = torch.float32,\n",
    "  batch_size = 1,\n",
    "  num_videos_per_prompt = 1,\n",
    "  do_classifier_free_guidance = True,\n",
    "):\n",
    "  add_time_ids = [fps, motion_bucket_id, noise_aug_strength]\n",
    "\n",
    "  add_time_ids = torch.tensor([add_time_ids], dtype=dtype)\n",
    "  add_time_ids = add_time_ids.repeat(batch_size * num_videos_per_prompt, 1)\n",
    "\n",
    "  if do_classifier_free_guidance:\n",
    "      add_time_ids = torch.cat([add_time_ids, add_time_ids])\n",
    "\n",
    "  return add_time_ids\n",
    "\n",
    "added_time_ids = get_add_time_ids(dtype=dtype).to(device)\n",
    "\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from packaging import version\n",
    "\n",
    "testing_unet = testing_unet.to(dtype=dtype, device=device)\n",
    "my_unet = my_unet.to(dtype=dtype, device=device)\n",
    "# move time to divice\n",
    "time = time.to(device)\n",
    "print(f\"Time is on: {time.device}\")\n",
    "from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n",
    "# if True:\n",
    "#     if is_xformers_available():\n",
    "#         import xformers\n",
    "\n",
    "#         xformers_version = version.parse(xformers.__version__)\n",
    "#         if xformers_version == version.parse(\"0.0.16\"):\n",
    "#             pass\n",
    "\n",
    "#         testing_unet.enable_xformers_memory_efficient_attention()\n",
    "#     else:\n",
    "#         raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n",
    "sample = torch.ones(2,14, 8, 36, 64)\n",
    "# sample = torch.ones(2, 4, 64, 64)\n",
    "sample = sample.to(dtype=dtype, device=device) \n",
    "time = time.to(dtype=dtype, device=device)\n",
    "added_time_ids = added_time_ids.to(dtype=dtype, device=device)  \n",
    "prompt_embeds = prompt_embeds.to(dtype=dtype, device=device)\n",
    "\n",
    "for p in my_unet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "noise_pred = my_unet.forward(\n",
    "    sample,\n",
    "    time,\n",
    "    added_time_ids=added_time_ids.to(dtype=dtype, device=device),\n",
    "    encoder_hidden_states = prompt_embeds.to(dtype=dtype, device=device),\n",
    "    # encoder_hidden_states = None,\n",
    "    return_dict=True,\n",
    "    # controlnet_condition = torch.ones(25, 4, 576, 1024).to(dtype=dtype, device=device)\n",
    ")    \n",
    "\n",
    "\n",
    "# Print the sizes of the tensors\n",
    "\n",
    "if noise_pred is not None:\n",
    "    del noise_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlnet Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 has 14 samples\n",
      "Caption: A driving scene during the day, with clear weather in singapore-onenorth\n",
      "Reference image shape: torch.Size([3, 320, 512])\n",
      "Conditioning image shape: torch.Size([14, 4, 320, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "\n",
    "class DiffusionDataset(Dataset):\n",
    "    def __init__(self, json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((320, 512)),\n",
    "            transforms.CenterCrop((320, 512)),\n",
    "        ])\n",
    "        self.image_processor = VaeImageProcessor(vae_scale_factor=8)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Assuming each set of ground truths represents a separate sample\n",
    "        return len(self.data['ground_truth'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Processing ground truth images\n",
    "        \n",
    "        ground_truth_images = [self.transform(Image.open(path)) for path in self.data['ground_truth'][idx]]\n",
    "        ground_truth_images = self.image_processor.preprocess(image = ground_truth_images, height = 320, width = 512)\n",
    "\n",
    "        # Processing conditioning images set one (assuming RGB, 4 channels after conversion)\n",
    "        conditioning_images_one = [self.transform(Image.open(path)) for path in self.data['conditioning_images_one'][idx]]\n",
    "        conditioning_images_one = self.image_processor.preprocess(image = conditioning_images_one, height = 320, width = 512)\n",
    "\n",
    "        # Processing conditioning images set two (assuming grayscale, converted to RGB to match dimensions)\n",
    "        conditioning_images_two = [self.transform(Image.open(path)) for path in self.data['conditioning_images_two'][idx]]\n",
    "        conditioning_images_two = self.image_processor.preprocess(image = conditioning_images_two, height = 320, width = 512)\n",
    "        \n",
    "        # Concatenating condition one and two images along the channel dimension\n",
    "        conditioned_images = [torch.cat((img_one, img_two), dim=0) for img_one, img_two in zip(conditioning_images_one, conditioning_images_two)]\n",
    "\n",
    "        # Processing reference images (single per scene, matched by index)\n",
    "        reference_image = self.transform(Image.open(self.data['reference_image'][idx][0]))\n",
    "\n",
    "        # Retrieving the corresponding caption\n",
    "        caption = self.data['caption'][idx][0]\n",
    "\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"ground_truth\": ground_truth_images,\n",
    "            \"conditioning\": torch.stack(conditioned_images),\n",
    "            \"caption\": caption,\n",
    "            \"reference_image\": reference_image\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ground_truth = torch.stack([item['ground_truth'] for item in batch])\n",
    "    conditioning = torch.stack([item['conditioning'] for item in batch])\n",
    "    captions = [item['caption'] for item in batch]  # List of strings, no need to stack\n",
    "    reference_images = [item['reference_image'] for item in batch]\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"ground_truth\": ground_truth.flatten(0, 1),\n",
    "        \"conditioning\": conditioning.flatten(0, 1),\n",
    "        \"caption\": captions[0],\n",
    "        \"reference_image\": reference_images[0],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = DiffusionDataset(json_path='/mnt/e/13_Jasper_diffused_samples/complete_data_paths.json')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=1,  # Or your preferred batch size\n",
    "    num_workers=0,  # Adjust based on your setup\n",
    ")\n",
    "\n",
    "for i, batch in enumerate(train_dataloader):\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    print(f\"Batch {i} has {batch['ground_truth'].shape[0]} samples\")\n",
    "    print(f\"Caption: {batch['caption']}\")\n",
    "    print(f\"Reference image shape: {to_tensor(batch['reference_image']).shape}\")\n",
    "    print(f\"Conditioning image shape: {batch['conditioning'].shape}\")\n",
    "\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'ignore_mismatched_sizes': False} are not expected by StableVideoDiffusionPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:01<00:00,  4.54it/s]\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00,  9.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize the contrl net from my_net\n",
    "\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet,SpatioTemporalControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n",
    "\n",
    "import gc\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# Importing the pipelines\n",
    "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid\", torch_dtype=torch.float16, variant=\"fp16\", ignore_mismatched_sizes=False\n",
    ")\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2\")\n",
    "\n",
    "\n",
    "# Getting the models\n",
    "tokenizer = pipeline.tokenizer\n",
    "text_encoder = pipeline.text_encoder\n",
    "unet_weights = pipe.unet.state_dict()\n",
    "testing_unet = UNetSpatioTemporalConditionModel()\n",
    "testing_unet.load_state_dict(unet_weights)\n",
    "control_net_blank = SpatioTemporalControlNet.from_unet(testing_unet).half()\n",
    "testing_unet = testing_unet.half()\n",
    "# dtype = torch.float16\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# vae = pipe.vae.to(dtype=dtype, device=device)\n",
    "# vae_output = pipe.vae.encode(batch['ground_truth'].to(dtype=dtype, device=device)).latent_dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# checkpoint = torch.load('/mnt/e/13_Jasper_diffused_samples/training/unet_with_scaling/test/model_checkpoint_5599.ckpt')\n",
    "\n",
    "\n",
    "from diffusers.pipelines.stable_video_diffusion.pipeline_stable_video_diffusion_with_controlnet import StableVideoDiffusionPipelineWithControlNet,SpatioTemporalControlNet, CustomConditioningNet, SpatioTemporalControlNetOutput\n",
    "# control_net_trained = SpatioTemporalControlNet()\n",
    "# unet_trained = UNetSpatioTemporalConditionModel()\n",
    "# print(checkpoint['model_state_dict'].keys())     \n",
    "# control_net_trained.load_state_dict(checkpoint['model_state_dict'])\n",
    "# unet_trained.load_state_dict(checkpoint['unet_state_dict'])\n",
    "# control_net_trained = control_net_trained.half()\n",
    "# unet_trained = unet_trained.half()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0292,  0.0292, -0.0232,  ...,  0.0070,  0.0068, -0.0293],\n",
      "        [ 0.0126, -0.0143,  0.0074,  ..., -0.0103,  0.0351, -0.0125],\n",
      "        [ 0.0272, -0.0305, -0.0247,  ..., -0.0238, -0.0183, -0.0316],\n",
      "        ...,\n",
      "        [-0.0057, -0.0232,  0.0119,  ...,  0.0057,  0.0217, -0.0027],\n",
      "        [ 0.0166, -0.0035, -0.0032,  ..., -0.0136,  0.0210, -0.0216],\n",
      "        [ 0.0050, -0.0297,  0.0295,  ...,  0.0205, -0.0130, -0.0165]],\n",
      "       dtype=torch.float16) tensor([[-1.8997e-02, -1.0264e-04,  3.2115e-04,  ...,  1.4046e-02,\n",
      "          2.2751e-02, -2.8667e-03],\n",
      "        [ 1.8875e-02, -2.8671e-02, -2.6703e-02,  ..., -3.2471e-02,\n",
      "         -1.5228e-02,  3.1982e-02],\n",
      "        [-1.3405e-02, -3.0869e-02, -2.0889e-02,  ...,  6.9962e-03,\n",
      "          1.5945e-02, -1.3306e-02],\n",
      "        ...,\n",
      "        [-2.4368e-02,  3.7556e-03, -7.7665e-05,  ..., -3.4210e-02,\n",
      "         -1.0155e-02,  5.1355e-04],\n",
      "        [-8.4543e-04,  1.2459e-02,  7.4310e-03,  ..., -3.2074e-02,\n",
      "          2.8763e-02,  9.1934e-03],\n",
      "        [ 1.6388e-02,  1.7105e-02,  1.0391e-02,  ..., -1.5305e-02,\n",
      "         -2.0390e-03, -3.1586e-02]], dtype=torch.float16)\n",
      "The models do not have the same weights.\n"
     ]
    }
   ],
   "source": [
    "# Assuming model1 and model2 are your neural network models in TensorFlow\n",
    "model1_state_dict  = control_net_blank.state_dict()\n",
    "model2_state_dict  = control_net_trained.state_dict()\n",
    "\n",
    "are_same = True\n",
    "for p1, p2 in zip(model1_state_dict.values(), model2_state_dict.values()):\n",
    "    if not torch.equal(p1, p2):\n",
    "        are_same = False\n",
    "        print(p1,p2)\n",
    "        break\n",
    "\n",
    "if are_same:\n",
    "    print(\"The models have the same weights.\")\n",
    "else:\n",
    "    print(\"The models do not have the same weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights_sum(model):\n",
    "    # Initialize a counter\n",
    "    total_weight_sum = 0.0\n",
    "\n",
    "    # Iterate over controlnet_down_blocks\n",
    "    for i, block in enumerate(model.down_blocks):\n",
    "        block_weight_sum = sum(p.data.sum() for p in block.parameters() if p.requires_grad)\n",
    "        print(f\"ControlNet Down Block {i} Weight Sum: {block_weight_sum}\")\n",
    "        total_weight_sum += block_weight_sum\n",
    "\n",
    "    # Check the controlnet_mid_block\n",
    "    mid_block_weight_sum = sum(p.data.sum() for p in model.controlnet_mid_block.parameters() if p.requires_grad)\n",
    "    print(f\"ControlNet Mid Block Weight Sum: {mid_block_weight_sum}\")\n",
    "    total_weight_sum += mid_block_weight_sum\n",
    "\n",
    "    # Print total sum of weights for a sanity check\n",
    "    print(f\"Total Weight Sum of Specified Layers: {total_weight_sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 weight sum: 0.0\n",
      "Layer 1 weight sum: 0.0\n",
      "Layer 2 weight sum: 0.0\n",
      "Layer 3 weight sum: 0.0\n",
      "Layer 4 weight sum: 0.0\n",
      "Layer 5 weight sum: 0.0\n",
      "Layer 6 weight sum: 0.0\n",
      "Layer 7 weight sum: 0.0\n",
      "Layer 8 weight sum: 0.0\n",
      "Layer 9 weight sum: 0.0\n",
      "Layer 10 weight sum: 0.0\n",
      "Layer 11 weight sum: 0.0\n",
      "Layer 12 weight sum: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Assuming `model` is your trained model instance\n",
    "def print_sum_of_weights_for_zero_initialized_layers(model):\n",
    "    layers_of_interest = [model.controlnet_mid_block] + list(model.controlnet_down_blocks)\n",
    "    for i, layer in enumerate(layers_of_interest):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            weight_sum = layer.weight.sum().item()\n",
    "            print(f\"Layer {i} weight sum: {weight_sum}\")\n",
    "# print_sum_of_weights_for_zero_initialized_layers(control_net_trained)\n",
    "print_sum_of_weights_for_zero_initialized_layers(control_net_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ControlNet Down Block 0 Weight Sum: -196.875\n",
      "ControlNet Down Block 1 Weight Sum: 4816.0\n",
      "ControlNet Down Block 2 Weight Sum: -3540.0\n",
      "ControlNet Down Block 3 Weight Sum: 212.0\n",
      "ControlNet Mid Block Weight Sum: 0.0\n",
      "Total Weight Sum of Specified Layers: 1292.0\n"
     ]
    }
   ],
   "source": [
    "# print_weights_sum(control_net_trained)\n",
    "print_weights_sum(control_net_blank)\n",
    "# print_weights_sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A driving scene during the night, with rainy weather in boston-seaport\"\n",
    "# prompt = batch['caption']\n",
    "pseudo_sample = batch['conditioning'][:14]\n",
    "# Define a simple torch generator\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "image = batch['reference_image']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "tset = torch.zeros(2,8)\n",
    "\n",
    "print(tset[1:, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'ignore_mismatched_sizes': False} are not expected by StableVideoDiffusionPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE scale factor: 8\n"
     ]
    }
   ],
   "source": [
    "# Without controlnet\n",
    "pipe_zero = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid\", torch_dtype=torch.float16, variant=\"fp16\", ignore_mismatched_sizes=False\n",
    ")\n",
    "\n",
    "# pipe_one = StableVideoDiffusionPipeline(\n",
    "#     vae = pipe.vae.to(dtype=torch.float16, device=torch.device(\"cuda\")),\n",
    "#     image_encoder = pipe.image_encoder.to(dtype=torch.float16, device=torch.device(\"cuda\")),\n",
    "#     unet=pipe_zero.unet.to(dtype=torch.float16, device=torch.device(\"cuda\")),\n",
    "#     scheduler=pipe.scheduler,\n",
    "#     feature_extractor=pipe.feature_extractor,\n",
    "# )\n",
    "\n",
    "# unet_weights = pipe.unet.state_dict()\n",
    "# unet_from_state_dict = UNetSpatioTemporalConditionModel()\n",
    "# unet_from_state_dict.load_state_dict(unet_weights)\n",
    "\n",
    "# pipe_two = StableVideoDiffusionPipeline(\n",
    "#     vae = pipe.vae.to(dtype=torch.float16, device=torch.device(\"cuda\")),\n",
    "#     image_encoder = pipe.image_encoder.to(dtype=torch.float16, device=torch.device(\"cuda\")),\n",
    "#     unet=unet_from_state_dict.to(dtype=torch.float16, device=torch.device(\"cuda\")),\n",
    "#     scheduler=pipe.scheduler,\n",
    "#     feature_extractor=pipe.feature_extractor,\n",
    "# )\n",
    "\n",
    "# with controlnet\n",
    "pipe_zero_with_controlnet = StableVideoDiffusionPipelineWithControlNet(\n",
    "    vae = pipe.vae,\n",
    "    image_encoder = pipe.image_encoder,\n",
    "    unet=pipe_zero.unet.to(dtype=torch.float16, device=torch.device(\"cuda\")),\n",
    "    scheduler=pipe.scheduler,\n",
    "    feature_extractor=pipe.feature_extractor,\n",
    "    controlnet=control_net_blank,\n",
    "    tokenizer = tokenizer,\n",
    "    text_encoder = text_encoder\n",
    ")\n",
    "\n",
    "# pipe_one_with_controlnet = StableVideoDiffusionPipelineWithControlNet(\n",
    "#     vae = pipe.vae,\n",
    "#     image_encoder = pipe.image_encoder,\n",
    "#     unet=pipe_one.unet.to(dtype=torch.float16, device=torch.device(\"cuda\")),\n",
    "#     scheduler=pipe.scheduler,\n",
    "#     feature_extractor=pipe.feature_extractor,\n",
    "#     controlnet=control_net_blank,\n",
    "#     tokenizer = tokenizer,\n",
    "#     text_encoder = text_encoder\n",
    "# )\n",
    "\n",
    "# pipe_two_with_controlnet = StableVideoDiffusionPipelineWithControlNet(\n",
    "#     vae = pipe.vae,\n",
    "#     image_encoder = pipe.image_encoder,\n",
    "#     unet=pipe_two.unet.to(dtype=torch.float16, device=torch.device(\"cuda\")),\n",
    "#     scheduler=pipe.scheduler,\n",
    "#     feature_extractor=pipe.feature_extractor,\n",
    "#     controlnet=control_net_blank,\n",
    "#     tokenizer = tokenizer,\n",
    "#     text_encoder = text_encoder\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_zero_with_controlnet = pipe_zero_with_controlnet.to(dtype=torch.float16, device=torch.device(\"cuda\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:14<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# frames = pipe(height=320,width=512, image=image,num_frames = 14,  decode_chunk_size=8, generator=generator).frames[0]\n",
    "# frames_zero = pipe_zero(height=288,width=512, image=image,num_frames = 14,  decode_chunk_size=8, generator=generator).frames[0]\n",
    "# frames_one = pipe_one(height=288,width=512, image=image,num_frames = 14,  decode_chunk_size=8, generator=generator).frames[0]\n",
    "# frames_two = pipe_two(height=288,width=512, image=image,num_frames = 14,  decode_chunk_size=8, generator=generator).frames[0]\n",
    "frames_zero_with_controlnet = pipe_zero_with_controlnet(height=320,width=512, image=image,num_frames = 14, prompt=prompt, conditioning_image = pseudo_sample,  decode_chunk_size=8, generator=generator).frames[0]\n",
    "# frames_one_with_controlnet = pipe_one_with_controlnet(height=288,width=512, image=image,num_frames = 14, prompt=prompt, conditioning_image = pseudo_sample,  decode_chunk_size=8, generator=generator).frames[0]\n",
    "# frames_two_with_controlnet = pipe_two_with_controlnet(height=288,width=512, image=image,num_frames = 14, prompt=prompt, conditioning_image = pseudo_sample,  decode_chunk_size=8, generator=generator).frames[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = batch['reference_image']\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the shape of the image torch.Size([3, 360, 640])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torchvision import transforms\n",
    "to_tensor = to_tensor = transforms.ToTensor()\n",
    "image = to_tensor(image)\n",
    "print(f'this is the shape of the image {image.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_vae_image(\n",
    "\n",
    "        image: torch.Tensor,\n",
    "        vae\n",
    "    ):\n",
    "\n",
    "        with torch.no_grad(): \n",
    "\n",
    "            # print(f\"this is the shape of the image: {image.shape}\")\n",
    "            image = image.to(device=vae.device, dtype=vae.dtype)\n",
    "            image_latents = vae.encode(image.to(device=vae.device)).latent_dist.mode()\n",
    "\n",
    "\n",
    "\n",
    "            # duplicate image_latents for each generation per prompt, using mps friendly method\n",
    "            image_latents = image_latents.repeat(1, 1, 1, 1)\n",
    "\n",
    "            return image_latents\n",
    "\n",
    "def encode_batch(images, vae ):\n",
    "    outputs = []  # Initialize an empty list to store each output\n",
    "\n",
    "\n",
    "    # Loop through each image in the pseudo_image tensor\n",
    "    for i in range(images.shape[0]):\n",
    "        output = _encode_vae_image(\n",
    "            images[i].unsqueeze(0),  # Unsqueeze to add the batch dimension back\n",
    "            vae=vae\n",
    "        )\n",
    "        outputs.append(output)  # Append the output to the list\n",
    "\n",
    "    # Concatenate all outputs along the 0 dimension\n",
    "    final_output = torch.cat(outputs, dim=0)\n",
    "    final_output = final_output.unsqueeze(0)\n",
    "\n",
    "    if True:\n",
    "        negative_image_latents = torch.zeros_like(final_output)\n",
    "\n",
    "        # For classifier free guidance, we need to do two forward passes.\n",
    "        # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "        # to avoid doing two forward passes\n",
    "        final_output = torch.cat([negative_image_latents, final_output])\n",
    "\n",
    "    return final_output\n",
    "\n",
    "output = encode_batch(batch['ground_truth'], pipe_with_controlnet.vae)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nono_noise.mp4'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export_to_video(frames_zero, \"frames_zero.mp4\", fps=7)\n",
    "# export_to_video(frames_one, \"frames_one.mp4\", fps=7)\n",
    "# export_to_video(frames_two, \"frames_two.mp4\", fps=7)\n",
    "# export_to_video(frames, \"frames_perfect_2.mp4\", fps=7)\n",
    "# export_to_video(frames_one_with_controlnet, \"frames_one_with_controlnet_condition.mp4\", fps=7)\n",
    "# export_to_video(frames_two_with_controlnet, \"frames_two_with_controlnet_condition.mp4\", fps=7)\n",
    "export_to_video(frames_zero_with_controlnet, \"nono_noise.mp4\", fps=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent model input dtype: torch.float32\n",
      "Hidden image embeddings dtype: torch.float32\n",
      "torch.Size([2, 1, 8, 64, 64])\n",
      "torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 3])\n",
      "Latent model input is on: cuda:0\n",
      "Hidden image embeddings are on: cuda:0\n",
      "Added time IDs are on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "def prepare_latents(\n",
    "    batch_size,\n",
    "    num_frames,\n",
    "    num_channels_latents,\n",
    "    height,\n",
    "    width,\n",
    "    dtype,\n",
    "    device,\n",
    "    generator,\n",
    "    latents=None,\n",
    "):\n",
    "    shape = (\n",
    "        batch_size,\n",
    "        num_frames,\n",
    "        num_channels_latents // 2,\n",
    "        height // 1,\n",
    "        width // 1,\n",
    "    )\n",
    "    if isinstance(generator, list) and len(generator) != batch_size:\n",
    "        raise ValueError(\n",
    "            f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "            f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "        )\n",
    "\n",
    "    if latents is None:\n",
    "        latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
    "    else:\n",
    "        latents = latents.to(device)\n",
    "\n",
    "    # scale the initial noise by the standard deviation required by the scheduler\n",
    "    latents = latents * 0.2\n",
    "    return latents\n",
    "\n",
    "def pseudo_image_embeddings( shape, generator, device, dtype, do_classifier_free_guidance = True ):\n",
    "    image_embeddings = randn_tensor(shape, generator=generator, device= device, dtype=dtype)\n",
    "\n",
    "    if do_classifier_free_guidance:\n",
    "        negative_image_embeddings = torch.zeros_like(image_embeddings)\n",
    "\n",
    "        # For classifier free guidance, we need to do two forward passes.\n",
    "        # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "        # to avoid doing two forward passes\n",
    "        return torch.cat([negative_image_embeddings, image_embeddings])\n",
    "\n",
    "def get_add_time_ids(\n",
    "  fps = 7,\n",
    "  motion_bucket_id = 127,\n",
    "  noise_aug_strength = 0.02,\n",
    "  dtype = torch.float32,\n",
    "  batch_size = 1,\n",
    "  num_videos_per_prompt = 1,\n",
    "  do_classifier_free_guidance = True,\n",
    "):\n",
    "  add_time_ids = [fps, motion_bucket_id, noise_aug_strength]\n",
    "\n",
    "  add_time_ids = torch.tensor([add_time_ids], dtype=dtype)\n",
    "  add_time_ids = add_time_ids.repeat(batch_size * num_videos_per_prompt, 1)\n",
    "\n",
    "  if do_classifier_free_guidance:\n",
    "      add_time_ids = torch.cat([add_time_ids, add_time_ids])\n",
    "\n",
    "  return add_time_ids\n",
    "\n",
    "dtype = torch.float32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)  # For reproducibility\n",
    "\n",
    "# Generate original latents with specified dtype\n",
    "my_latents = prepare_latents(1, 1, 8, 64, 64, dtype, device, generator)\n",
    "\n",
    "# Apply classifier-free guidance by duplicating the latents and ensuring the correct dtype\n",
    "latent_model_input = torch.cat([my_latents] * 2)  # Inherits dtype from my_latents\n",
    "\n",
    "# Create pseudo image latents by cloning the original latents\n",
    "pseudo_image_latents = latent_model_input.clone()  # Inherits dtype\n",
    "\n",
    "# Concatenate pseudo image latents over the channels dimension, ensuring matching dtype\n",
    "latent_model_input = torch.cat([latent_model_input, pseudo_image_latents], dim=2)\n",
    "\n",
    "\n",
    "# Create the fake image embeddings with the specified dtype\n",
    "hidden_image_embeddings = pseudo_image_embeddings((1, 1, 1024), generator, device, dtype)\n",
    "\n",
    "added_time_ids = get_add_time_ids(dtype=dtype).to(device)\n",
    "\n",
    "# Verify the dtype of both tensors\n",
    "print(f\"Latent model input dtype: {latent_model_input.dtype}\")\n",
    "print(f\"Hidden image embeddings dtype: {hidden_image_embeddings.dtype}\")\n",
    "\n",
    "print(latent_model_input.shape)\n",
    "print(hidden_image_embeddings.shape)\n",
    "print(added_time_ids.shape)\n",
    "\n",
    "# Print on which model they are\n",
    "print(f\"Latent model input is on: {latent_model_input.device}\")\n",
    "print(f\"Hidden image embeddings are on: {hidden_image_embeddings.device}\")\n",
    "# Assuming added_time_ids is also a tensor; replace this with the actual tensor variable if different\n",
    "print(f\"Added time IDs are on: {added_time_ids.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Float and Half",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 3\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_image_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43madded_time_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_time_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdown_block_additional_residuals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmid_block_additional_residual\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(noise_pred\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m noise_pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/models/unets/unet_spatio_temporal_condition.py:421\u001b[0m, in \u001b[0;36mUNetSpatioTemporalConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, added_time_ids, return_dict, down_block_additional_residuals, mid_block_additional_residual)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# `Timesteps` does not contain any weights and will always return f32 tensors\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# but time_embedding might actually be running in fp16. so we need to cast here.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# there might be better ways to encapsulate this.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m t_emb\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39msample\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 421\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m time_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_time_proj(added_time_ids\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m    424\u001b[0m time_embeds \u001b[38;5;241m=\u001b[39m time_embeds\u001b[38;5;241m.\u001b[39mreshape((batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/models/embeddings.py:228\u001b[0m, in \u001b[0;36mTimestepEmbedding.forward\u001b[0;34m(self, sample, condition)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     sample \u001b[38;5;241m=\u001b[39m sample \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_proj(condition)\n\u001b[0;32m--> 228\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(sample)\n",
      "File \u001b[0;32m~/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/diffusers/models/lora.py:430\u001b[0m, in \u001b[0;36mLoRACompatibleLinear.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, scale: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/wsl_diffusers/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Float and Half"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    noise_pred = testing_unet.forward(\n",
    "        latent_model_input.to(dtype=dtype),\n",
    "        torch.tensor(1).to(dtype=dtype, device=device),\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        down_block_additional_residuals= None,\n",
    "        mid_block_additional_residual = None,\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "    print(noise_pred.shape)\n",
    "    if noise_pred is not None:\n",
    "        del noise_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of prompt embeds: torch.Size([1, 77, 1024]) 1 77\n",
      "torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "from typing import Optional, List\n",
    "\n",
    "def encode_prompt(\n",
    "    prompt,\n",
    "    device,\n",
    "    do_classifier_free_guidance,\n",
    "    negative_prompt=\"Simulation, artifacts, blurry, low resolution, low quality, noisy, grainy, distorted\",\n",
    "    num_images_per_prompt = 1,\n",
    "    prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "    lora_scale: Optional[float] = None,\n",
    "    clip_skip: Optional[int] = None,\n",
    "    text_encoder = None, \n",
    "    tokenizer = None):\n",
    "    # Set the text_encoder and the tokenizer on the correct device  \n",
    "    text_encoder = text_encoder.to(device)\n",
    "\n",
    "\n",
    "    if prompt is not None and isinstance(prompt, str):\n",
    "        batch_size = 1\n",
    "    elif prompt is not None and isinstance(prompt, list):\n",
    "        batch_size = len(prompt)\n",
    "    else:\n",
    "        batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "    if True:\n",
    "\n",
    "        text_inputs = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "        untruncated_ids = tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "        if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(\n",
    "            text_input_ids, untruncated_ids\n",
    "        ):\n",
    "            removed_text = tokenizer.batch_decode(\n",
    "                untruncated_ids[:, tokenizer.model_max_length - 1 : -1]\n",
    "            )\n",
    "            logger.warning(\n",
    "                \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
    "                f\" {tokenizer.model_max_length} tokens: {removed_text}\"\n",
    "            )\n",
    "\n",
    "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "            attention_mask = text_inputs.attention_mask.to(device)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "\n",
    "        prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=attention_mask)\n",
    "        prompt_embeds = prompt_embeds[0]\n",
    "       \n",
    "\n",
    "    if text_encoder is not None:\n",
    "        prompt_embeds_dtype = text_encoder.dtype\n",
    "\n",
    "\n",
    "    prompt_embeds = prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "\n",
    "    bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "    print(f\"Shape of prompt embeds: {prompt_embeds.shape} {bs_embed} {seq_len}\")\n",
    "    \n",
    "    # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "    prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "    prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "    # get unconditional embeddings for classifier free guidance\n",
    "    if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
    "        uncond_tokens: List[str]\n",
    "        if negative_prompt is None:\n",
    "            uncond_tokens = [\"\"] * batch_size\n",
    "        elif prompt is not None and type(prompt) is not type(negative_prompt):\n",
    "            raise TypeError(\n",
    "                f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                f\" {type(prompt)}.\"\n",
    "            )\n",
    "        elif isinstance(negative_prompt, str):\n",
    "            uncond_tokens = [negative_prompt]\n",
    "        elif batch_size != len(negative_prompt):\n",
    "            raise ValueError(\n",
    "                f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                \" the batch size of `prompt`.\"\n",
    "            )\n",
    "        else:\n",
    "            uncond_tokens = negative_prompt\n",
    "        \n",
    "        max_length = prompt_embeds.shape[1]\n",
    "        uncond_input = tokenizer(\n",
    "            uncond_tokens,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "            attention_mask = uncond_input.attention_mask.to(device)\n",
    "        else:\n",
    "            attention_mask = None\n",
    "\n",
    "        negative_prompt_embeds = text_encoder(\n",
    "            uncond_input.input_ids.to(device),\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        negative_prompt_embeds = negative_prompt_embeds[0]\n",
    "\n",
    "    if do_classifier_free_guidance:\n",
    "        # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "        seq_len = negative_prompt_embeds.shape[1]\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.to(dtype=prompt_embeds_dtype, device=device)\n",
    "\n",
    "        negative_prompt_embeds = negative_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        negative_prompt_embeds = negative_prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "\n",
    "    \n",
    "    embeds = torch.cat([prompt_embeds, negative_prompt_embeds])\n",
    "    embeds = embeds.mean(dim=1, keepdim=True)\n",
    "\n",
    "    return embeds\n",
    "\n",
    "prompt = \"A f a cat\"\n",
    "prompt_embeds = encode_prompt(prompt=prompt, device=device, do_classifier_free_guidance=True, text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "print(prompt_embeds.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have a sample of the following dimensions: sample = torch.ones(2,25, 8, 72, 128), because this sample is abit to big I would like to downsampel to the following dimernesions\n",
    "sample = torch.ones(2,25, 8, 36, 64) please help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 25, 8, 36, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Define the original dimensions\n",
    "original_dimensions = (2, 25, 8, 72, 128)\n",
    "\n",
    "# Define the target dimensions\n",
    "target_dimensions = (2, 25, 8, 36, 64)\n",
    "\n",
    "# Create a sample tensor with the original dimensions\n",
    "sample = torch.ones(original_dimensions)\n",
    "\n",
    "# Downsample the sample tensor to the target dimensions using interpolation\n",
    "downsampled_sample = torch.nn.functional.interpolate(sample, size=target_dimensions[2:] ,mode='nearest')\n",
    "\n",
    "# Check the dimensions of the downsampled sample tensor\n",
    "print(downsampled_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of down_block_res_samples afterprocesssing: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dtype = torch.float16\n",
    "testing_unet.requires_grad_(False)\n",
    "\n",
    "for p in testing_unet.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "time =  torch.tensor(1).to(dtype=dtype, device=device)\n",
    "if True:\n",
    "    \n",
    "    sample = torch.ones(2,14, 8, 36, 64).to(dtype=dtype, device=device)    \n",
    "\n",
    "    # first resize to (50,8,72,128)\n",
    "    # sample_control = sample.resize(50,8,72,128)\n",
    "    # sample_downsampled = torch.nn.functional.interpolate(sample_control, scale_factor=0.5, mode='nearest')\n",
    "\n",
    "    # movce back\n",
    "\n",
    "    (down_block_res_samples, mid_block_res_samples) = control_net_blank.forward(\n",
    "        sample.to(dtype=dtype),\n",
    "        time,\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    # reverse the down_block_res_samples tuple\n",
    "    # down_block_res_samples = down_block_res_samples[::-1]\n",
    "\n",
    "    print(f\"Length of down_block_res_samples afterprocesssing: {len(down_block_res_samples)}\")\n",
    "\n",
    "\n",
    "    noise_pred = testing_unet.forward(\n",
    "        sample.to(dtype=dtype),\n",
    "        torch.tensor(1).to(dtype=dtype, device=device),\n",
    "        encoder_hidden_states=hidden_image_embeddings.to(dtype=dtype),\n",
    "        # Maybe I need to reverse the order of the tensors\n",
    "        down_block_additional_residuals= down_block_res_samples,\n",
    "        mid_block_additional_residual = mid_block_res_samples,\n",
    "        added_time_ids=added_time_ids.to(dtype=dtype),\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "    # Print the sizes of the tensors\n",
    "    if noise_pred is not None:\n",
    "        del noise_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 25, 320, 72, 128])\n"
     ]
    }
   ],
   "source": [
    "conditioning_net = CustomConditioningNet()\n",
    "pseudo_sample = torch.randn(25, 4, 578, 1028)\n",
    "hoi = conditioning_net.forward(pseudo_sample)\n",
    "print(hoi.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edit_diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
